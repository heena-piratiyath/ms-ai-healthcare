# -*- coding: utf-8 -*-
"""Self learningTutorial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J1ayuKdnEgktFrTConWOnMorZHQs9DlT?resourcekey=0-mawGq22Cx668yEN29fIMKg
"""

# Import Pandas library
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from textblob import TextBlob
from google.colab import files

# Load joined patients admission data
d1 = files.upload()
admissions_df = pd.read_csv('99591_patientAdmissions_icustays.csv')

# Load note data
d2 = files.upload()
notes_df = pd.read_csv('99591_Sepsis_Notes.csv')

# Load note data
d3 = files.upload()
labevents_df = pd.read_csv('99591_labevents.csv')

# Load note data
d4 = files.upload()
chartevents_df = pd.read_csv('99591_chartevents.csv')

# Load note data
d5 = files.upload()
microevents_df = pd.read_csv('99591_microevents.csv')

# Load note data
d6 = files.upload()
prescriptions_df = pd.read_csv('99591_prescriptions.csv')

admissions_df.info()
#admissions_df.iloc[0]
#print(len(admissions_df))

admissions_df.info()

!pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import pandas as pd

# Load the dataset
df = pd.read_csv('99591_Sepsis_Notes.csv')

# Drop rows with nulls in specified columns
df.dropna(subset=['TEXT'], inplace=True)

# Initialize the sentiment analyzer
analyzer = SentimentIntensityAnalyzer()

# Function to get compound sentiment scores
def get_compound_sentiment(text):
    return analyzer.polarity_scores(text)['compound']

# Apply the function to get compound sentiment scores for each text
df['sentiment_score'] = df['TEXT'].apply(get_compound_sentiment)

# Function to get sentiment based on compound score
def get_sentiment(score):
    if score >= 0.05:
        return 'Positive'
    elif score <= -0.05:
        return 'Negative'
    else:
        return 'Neutral'

# Apply the function to get sentiment for each note
df['sentiment'] = df['sentiment_score'].apply(get_sentiment)

# Display the first 5 rows
print(df[['TEXT', 'sentiment']].head().to_markdown(index=False, numalign="left", stralign="left"))

# Select relevant features
selected_features = [
    'LOS', 'ADMISSION_TYPE', 'ADMISSION_LOCATION',
    'DISCHARGE_LOCATION', 'INSURANCE', 'LANGUAGE', 'RELIGION',
    'MARITAL_STATUS', 'ETHNICITY'
]

# Target variable
target = 'HOSPITAL_EXPIRE_FLAG'

#print(admissions_df.columns)

# Prepare data for modeling
data = admissions_df[selected_features + [target]].copy()

data['LOS'].fillna(data['LOS'].median(), inplace=True)

# Categorical features: fill with mode
for col in data.columns:
    if data[col].dtype == 'object':
        data[col].fillna(data[col].mode()[0], inplace=True)

# Convert categorical features to numerical using one-hot encoding
categorical_cols = [col for col in data.columns if data[col].dtype == 'object']
data = pd.get_dummies(data, columns=categorical_cols, dummy_na=False)

# Split data into training and testing sets
X = data.drop(target, axis=1)
y = data[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build and train a Random Forest Classifier model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Evaluate the model
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
print(classification_report(y_test, y_pred))

"""# **Neural Network for Length of Stay Prediction**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt  # Import matplotlib for plotting
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Load the datasets
patients_df = pd.read_csv("99591_patientAdmissions_icustays.csv")
chartevents_df = pd.read_csv("99591_chartevents.csv")
labevents_df = pd.read_csv("99591_labevents.csv")

# Convert time columns to datetime
patients_df['ADMITTIME'] = pd.to_datetime(patients_df['ADMITTIME'])
patients_df['DISCHTIME'] = pd.to_datetime(patients_df['DISCHTIME'])  # Convert DISCHTIME
chartevents_df['CHARTTIME'] = pd.to_datetime(chartevents_df['CHARTTIME'])
labevents_df['CHARTTIME'] = pd.to_datetime(labevents_df['CHARTTIME'])

# Pivot chartevents and labevents to have ITEMIDs as columns
chartevents_pivot = chartevents_df.pivot_table(
    index=['SUBJECT_ID', 'HADM_ID', 'CHARTTIME'],
    columns='ITEMID',
    values='VALUENUM'
).reset_index()
labevents_pivot = labevents_df.pivot_table(
    index=['SUBJECT_ID', 'HADM_ID', 'CHARTTIME'],
    columns='ITEMID',
    values='VALUENUM'
).reset_index()

# Merge the pivoted DataFrames
combined_data = pd.merge(
    chartevents_pivot,
    labevents_pivot,
    on=['SUBJECT_ID', 'HADM_ID', 'CHARTTIME'],
    how='outer'
)

# Merge with patient data to get admission time and outcomes
# Now, HOSPITAL_EXPIRE_FLAG is directly used as the outcome
data = pd.merge(
    combined_data,
    patients_df[['SUBJECT_ID', 'HADM_ID', 'ADMITTIME', 'DISCHTIME']],
    on=['SUBJECT_ID', 'HADM_ID'],
    how='left'
)

# Sort by SUBJECT_ID, HADM_ID, and CHARTTIME
data = data.sort_values(by=['SUBJECT_ID', 'HADM_ID', 'CHARTTIME'])

# Handle missing values (a more robust approach)
# Fill missing values within each group (patient)
#data = data.groupby(['SUBJECT_ID', 'HADM_ID']).ffill()
# Fill remaining NaNs (at the beginning of groups or if no prior value) with 0
data = data.fillna(0)

# --- Reset the index to make 'SUBJECT_ID', 'HADM_ID', 'CHARTTIME' regular columns ---
data = data.reset_index(drop=True)

# Identify columns that are numeric and not the target
numerical_columns = data.select_dtypes(include=np.number).columns.tolist()
numerical_features = [
    col for col in numerical_columns if col != 'DISCHTIME'
]  # Exclude DISCHTIME

# --- Ensure all numerical_features are strings and exist in the DataFrame ---
numerical_features = [str(col) for col in numerical_features if str(col) in data.columns]

# Scale numerical features
scaler = StandardScaler()
data[numerical_features] = scaler.fit_transform(data[numerical_features])


# --- 1. Define Length of Stay ---
# Convert DISCHTIME and ADMITTIME to datetime if they aren't already
data['DISCHTIME'] = pd.to_datetime(data['DISCHTIME'])
data['ADMITTIME'] = pd.to_datetime(data['ADMITTIME'])

# Calculate the difference and convert to days
data['LengthOfStay'] = (data['DISCHTIME'] - data['ADMITTIME']).dt.total_seconds() / (
    60 * 60 * 24
)  # Length of stay in days

# --- Prepare data for MLP ---
# For a basic MLP, we'll aggregate the time-series data
# by taking the mean for each patient.
# In a real scenario, you'd need more sophisticated
# feature engineering to represent patient state.
aggregated_data = data.groupby(['SUBJECT_ID', 'HADM_ID'])[
    numerical_features + ['LengthOfStay']
].mean()

# --- Reset the index and handle existing columns ---
#aggregated_data = aggregated_data.reset_index()  # Remove SUBJECT_ID and HADM_ID
#aggregated_data['SUBJECT_ID'] = data['SUBJECT_ID'].unique()
#aggregated_data['HADM_ID'] = data['HADM_ID'].unique()

X = aggregated_data[numerical_features].values
y = aggregated_data['LengthOfStay'].values  # Use 'LengthOfStay' as the target

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# --- 4. Model Development ---
# Define the Neural Network model
model = Sequential()
model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(1, activation='linear'))  # Output layer for continuous LOS

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])  # Use MSE for regression

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(
    monitor='val_loss', patience=10, restore_best_weights=True
)  # Increased patience

# Train the model
history = model.fit(
    X_train,
    y_train,
    epochs=30,
    batch_size=64,
    validation_split=0.2,
    callbacks=[early_stopping],
)  # Increased epochs and batch size

# --- 5. Model Training and Evaluation ---
# Evaluate the model
loss, mae = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss}")
print(f"Test MAE: {mae}")

# --- 6. Plot Training History ---
# Plot training loss and validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training History: Loss')
plt.legend()
plt.show()

# Plot training MAE and validation MAE
plt.plot(history.history['mae'], label='Training MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.title('Training History: MAE')
plt.legend()
plt.show()

"""# **Feature Engineering and Model Improvement**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Load the datasets
patients_df = pd.read_csv("99591_patientAdmissions_icustays.csv")
chartevents_df = pd.read_csv("99591_chartevents.csv")
labevents_df = pd.read_csv("99591_labevents.csv")
prescriptions_df = pd.read_csv("99591_prescriptions.csv")
microevents_df = pd.read_csv("99591_microevents.csv")

# Convert time columns to datetime
patients_df['ADMITTIME'] = pd.to_datetime(patients_df['ADMITTIME'])
patients_df['DISCHTIME'] = pd.to_datetime(patients_df['DISCHTIME'])
chartevents_df['CHARTTIME'] = pd.to_datetime(chartevents_df['CHARTTIME'])
labevents_df['CHARTTIME'] = pd.to_datetime(labevents_df['CHARTTIME'])
prescriptions_df['STARTDATE'] = pd.to_datetime(prescriptions_df['STARTDATE'])
prescriptions_df['ENDDATE'] = pd.to_datetime(prescriptions_df['ENDDATE'])
microevents_df['CHARTTIME'] = pd.to_datetime(microevents_df['CHARTTIME'])

# Pivot chartevents and labevents to have ITEMIDs as columns
chartevents_pivot = chartevents_df.pivot_table(
    index=['SUBJECT_ID', 'HADM_ID', 'CHARTTIME'],
    columns='ITEMID',
    values='VALUENUM'
).reset_index()
labevents_pivot = labevents_df.pivot_table(
    index=['SUBJECT_ID', 'HADM_ID', 'CHARTTIME'],
    columns='ITEMID',
    values='VALUENUM'
).reset_index()

# Merge the pivoted DataFrames
combined_data = pd.merge(
    chartevents_pivot,
    labevents_pivot,
    on=['SUBJECT_ID', 'HADM_ID', 'CHARTTIME'],
    how='outer'
)

# Merge with patient data to get admission time and outcomes
data = pd.merge(
    combined_data,
    patients_df[['SUBJECT_ID', 'HADM_ID', 'ADMITTIME', 'DISCHTIME']],
    on=['SUBJECT_ID', 'HADM_ID'],
    how='left'
)

# Sort by SUBJECT_ID, HADM_ID, and CHARTTIME
data = data.sort_values(by=['SUBJECT_ID', 'HADM_ID', 'CHARTTIME'])

# --- Handle missing values (more robust approach) ---
# Fill missing values within each group (patient)
#data = data.groupby(['SUBJECT_ID', 'HADM_ID']).ffill()
# Fill remaining NaNs (at the beginning of groups or if no prior value) with 0
data = data.fillna(0)

# --- Reset the index to make 'SUBJECT_ID', 'HADM_ID', 'CHARTTIME' regular columns ---
data = data.reset_index(drop=True)

# --- Ensure DISCHTIME and ADMITTIME are datetime objects before calculating LengthOfStay ---
data['DISCHTIME'] = pd.to_datetime(data['DISCHTIME'])
data['ADMITTIME'] = pd.to_datetime(data['ADMITTIME'])

# --- 1. Define Length of Stay ---
data['LengthOfStay'] = (data['DISCHTIME'] - data['ADMITTIME']).dt.total_seconds() / (
    60 * 60 * 24
)  # Length of stay in days

# --- 2. Feature Engineering ---

# --- a. Windowed Time Aggregation ---
def aggregate_time_windows(df, time_windows):
    """
    Aggregates time-series data over specified time windows.

    Args:
        df: DataFrame with time-series data.
        time_windows: List of time window definitions (tuples of (start, end) in hours).

    Returns:
        DataFrame with aggregated features.
    """

    aggregated_features = pd.DataFrame()
    for start, end in time_windows:
        window_df = df[
            (df['CHARTTIME'] >= df['ADMITTIME'] + pd.Timedelta(hours=start))
            & (df['CHARTTIME'] <= df['ADMITTIME'] + pd.Timedelta(hours=end))
        ].copy()

        # Calculate aggregations
        window_agg = window_df.groupby(['SUBJECT_ID', 'HADM_ID']).agg(
            ['mean', 'min', 'max', 'std']
        )  # Add more aggregations as needed

        window_agg.columns = [
            '_'.join(map(str, col)).strip() for col in window_agg.columns.values
        ]
        window_agg = window_agg.add_prefix(f'window_{start}_{end}_')
        aggregated_features = pd.concat(
            [aggregated_features, window_agg], axis=1
        )
    return aggregated_features.reset_index()


# Define time windows (in hours)
time_windows = [(0, 24), (24, 48), (48, 72)]  # Example windows

# Aggregate data over time windows
aggregated_time_features = aggregate_time_windows(data, time_windows)

# --- b. Additional Features (Example: Age) ---
# Assuming you have age or a way to calculate it in patients_df
# For demonstration, let's create a dummy 'Age' feature
patients_df['Age'] = np.random.randint(18, 80, len(patients_df))  # Replace with actual age calculation

# Merge aggregated time features and patient demographics
feature_data = pd.merge(
    aggregated_time_features,
    patients_df[['SUBJECT_ID', 'HADM_ID', 'Age']],
    on=['SUBJECT_ID', 'HADM_ID'],
    how='left',
)

# Merge LengthOfStay from the original data
feature_data = pd.merge(
    feature_data,
    data[['SUBJECT_ID', 'HADM_ID', 'LengthOfStay']].drop_duplicates(),  # Drop duplicates to avoid errors
    on=['SUBJECT_ID', 'HADM_ID'],
    how='left'
)


# --- Prepare data for MLP ---
# Handle missing values in feature data
feature_data = feature_data.fillna(0)  # Or use a more sophisticated imputation

# Identify numerical features (excluding target and IDs)
numerical_cols = feature_data.select_dtypes(include=np.number).columns.tolist()
numerical_features = [
    col
    for col in numerical_cols
    if col not in ['SUBJECT_ID', 'HADM_ID', 'LengthOfStay']
]

# Scale numerical features
scaler = StandardScaler()
feature_data[numerical_features] = scaler.fit_transform(
    feature_data[numerical_features]
)

# Use feature_data as X and LengthOfStay as y
X = feature_data.groupby(['SUBJECT_ID', 'HADM_ID'])[numerical_features].mean().values
y = feature_data.groupby(['SUBJECT_ID', 'HADM_ID'])['LengthOfStay'].mean().values

# Reset index of y to match X
#y = y.reset_index(drop=True)
#y = pd.Series(y).reset_index(drop=True)

print(X.shape, y.shape)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# --- 4. Model Development ---
# Define the Neural Network model
model = Sequential()
model.add(Dense(256, input_dim=X_train.shape[1], activation='relu'))  # Increased units
model.add(Dropout(0.4))  # Increased dropout
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.4))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='linear'))

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(
    monitor='val_loss', patience=15, restore_best_weights=True
)  # Increased patience

# Train the model
history = model.fit(
    X_train,
    y_train,
    epochs=50,
    batch_size=64,
    validation_split=0.2,
    callbacks=[early_stopping],
)  # Increased epochs

# --- 5. Model Training and Evaluation ---
# Evaluate the model
loss, mae = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss}")
print(f"Test MAE: {mae}")

# --- 6. Plot Training History ---
# Plot training loss and validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training History: Loss')
plt.legend()
plt.show()

# Plot training MAE and validation MAE
plt.plot(history.history['mae'], label='Training MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.title('Training History: MAE')
plt.legend()
plt.show()

!pip install lifelines --upgrade

"""# **Survival Analysis**

*For analyzing time-to-event data, such as time until discharge or time until death.*
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from lifelines import KaplanMeierFitter
from lifelines import CoxPHFitter

# Load patient data
patients_df = pd.read_csv("99591_patientAdmissions_icustays.csv")

# Convert time columns to datetime
patients_df['ADMITTIME'] = pd.to_datetime(patients_df['ADMITTIME'])
patients_df['DISCHTIME'] = pd.to_datetime(patients_df['DISCHTIME'])

# Calculate Length of Stay (survival time) in days
patients_df['LengthOfStay'] = (
    patients_df['DISCHTIME'] - patients_df['ADMITTIME']
).dt.total_seconds() / (60 * 60 * 24)

# Create a 'status' variable:
#   - 1 if the event (discharge) was observed
#   - 0 if the event was censored (e.g., patient still in hospital)
patients_df['STATUS'] = 1  # Assuming all discharges are observed

# For demonstration, let's create some dummy covariates (features)
# You would replace these with actual relevant features
patients_df['Age'] = np.random.randint(18, 80, len(patients_df))
patients_df['Gender'] = np.random.choice(['M', 'F'], len(patients_df))

# Prepare data: Select survival time, status, and covariates
survival_data = patients_df[['LengthOfStay', 'STATUS', 'Age', 'Gender']].copy()

# Handle categorical variables
survival_data = pd.get_dummies(
    survival_data, columns=['Gender'], drop_first=True
)

# Kaplan-Meier Estimator
kmf = KaplanMeierFitter()
kmf.fit(
    durations=survival_data['LengthOfStay'], event_observed=survival_data['STATUS']
)

# Plot the survival curve
plt.figure(figsize=(8, 6))
kmf.plot_survival_function()
plt.title('Kaplan-Meier Survival Curve')
plt.xlabel('Time (Days)')
plt.ylabel('Survival Probability')
plt.grid(True)
plt.show()

# You can get median survival time
median_survival_time = kmf.median_survival_time_
print(f"Median Survival Time: {median_survival_time}")

# Cox Proportional Hazards Model
cph = CoxPHFitter()
cph.fit(survival_data, duration_col='LengthOfStay', event_col='STATUS')

# Print the summary of the Cox model
cph.print_summary()

# Visualize the impact of covariates

# Example: Visualize survival curves for different genders

# Kaplan-Meier curves for males
kmf_male = KaplanMeierFitter()
male_data = survival_data[survival_data['Gender_M'] == 1]
kmf_male.fit(male_data['LengthOfStay'], male_data['STATUS'], label='Male')
kmf_male.plot_survival_function()

# Kaplan-Meier curves for females
kmf_female = KaplanMeierFitter()
female_data = survival_data[survival_data['Gender_M'] == 0]
kmf_female.fit(female_data['LengthOfStay'], female_data['STATUS'], label='Female')
kmf_female.plot_survival_function()

plt.title('Survival Curves by Gender')
plt.xlabel('Time (Days)')
plt.ylabel('Survival Probability')
plt.grid(True)
plt.show()


# Create age groups (example)
survival_data['AgeGroup'] = pd.cut(survival_data['Age'], bins=[18, 40, 60, 80], labels=['18-40', '40-60', '60-80'], right=False)
age_groups = survival_data['AgeGroup'].unique()

plt.figure(figsize=(10, 6))
for group in age_groups:
    kmf = KaplanMeierFitter()
    group_data = survival_data[survival_data['AgeGroup'] == group]
    kmf.fit(group_data['LengthOfStay'], group_data['STATUS'], label=f'Age {group}')
    kmf.plot_survival_function()

plt.title('Survival Curves by Age Group')
plt.xlabel('Time (Days)')
plt.ylabel('Survival Probability')
plt.grid(True)
plt.show()